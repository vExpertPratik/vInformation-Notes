NSX 4.0	-> Merged Management & Control Plane
	-> KVM Hosts not supported
	-> Corfu DB is present with synchronous replication in NSX Manager with latency of 10ms

#============================================================================================================================================================================#

Virtual IP	-> In-Built 	-> You can set the virtual IP Address using NSX Manaager GUI
				-> All Manager should be part of same subnet
				-> It does not work as Load Balancer but the VIP will be mapped to one of the NSX Manaager
		-> External	-> You need external Load balancer (Ex. NSX ALB)
				-> All NSX Managers can be part of different subnets
				-> Balances the load as per the hashing algorithm set in LB

#============================================================================================================================================================================#

NSX Manager Size	-> Small [4vCPU, 16GB RAM, 300GB Storage]	-> POC
			-> Medium [6vCPU, 24GB RAM, 300GB Storage]	-> Less than 128 Transport Nodes
			-> Large [12vCPU, 48GB RAM, 300GB Storage]	-> Equal to or more than 128 Transport Nodes

#============================================================================================================================================================================#

NSX Manager Cluster Deployment methods
	With Compute Manager	-> Deploy first NSX Manager
				-> Add your vCenter environment as compute manager
				-> Deploy additional Appliances on vCenter Environment

	Without Compute Manager	-> Deploy all the 3 NSX Managers as standalone single node NSX Manager Cluster
				-> You will not be able to perform it via GUI as it requires Compute Manager
				-> Hence we will follow CLI Method
				-> Take Putty session of the 1st NSX Manager
					-> Run command "get cluster status" to get the Cluster-ID
					-> Run command "get certificate api thumbprint" to get Thumbprint
				-> Take Putty session of the 2nd NSX Manager
					-> Run below command
					-> Join <1st_NSX_Manager_IP> cluster-id <Cluster-ID> thumbprint <Thumbprint> username <username> password <password>
				-> By doing this you will erase all the data from 2nd NSX Manager and add it in a cluster with 1st NSX Manager
				-> Wait until all the respective services are up and cluster status is stable
				-> You can check the cluster status using "get cluster status"
				-> Once cluster is stable, you can perform the same steps for 3rd NSX Manager

#============================================================================================================================================================================#

NSX Manager Services which are affects the stability of the NSX Manager Cluster
	-> Datastore
	-> Cluster_Boot_Manager
	-> Controller
	-> Manager
	-> HTTPs
	-> Messaging Manager
	-> Site Manager
	-> Monitoring
	-> IDPS Reporting
	-> CM Inventory
	-> Corfu Nonconfig

#============================================================================================================================================================================#


Reverse Proxy Service -> NSX Manager service for registering Transport Node
After NSX 3.0, vDS can use with NSX if vSphere and vDS version is 7 or newer.
For Overlay traffic, MTU on vDS need to be at least 1600, 1700 (Recommended) or 9000 (Jumbo Frame)
No. of Uplinks and Teaming policy defines the number of TEPs created for Transport Node
			-> Failover -> Only one TEP
			-> Load Balance Source -> Balance as per vDS Port ID -> Active - Active -> Multiple TEPs
			-> Load Balance Source - MAC -> Balance as per MAC Hash -> High Compute
esxcli software vib list | grep -E 'nsx|vsip'
vmkping -I (Interface) -s (Size of Package) -d (not defragment) -S vxlan (type of packate) (destination)
Three NSX Forwarding Tables
	-> TEP Table -> Segment VNI to Host TEP IP Address Mapping
	-> ARP Table -> VM IP to MAC Mapping
	-> MAC Table -> VM MAC to Host TEP IP Address Mapping
Intratier Transit Link -> T0 to T1
If T0 HA Mode is active active, then we can not run centralized services (NAT, VPN, Firewall)
The Data Plane Development Kit (DPDK) is a set of data plane libraries and network interface controller drivers for fast packet processing
DFW 	-> Ethernet -> L2 Rule
	-> Emergency -> Quaratine Purpose
	-> Infrastructure -> DNS, DHCP, AD
	-> Environment -> based on type (Prod, Non-Prod)
	-> Application -> Layer 7
	-> on ESXi Host
		-> summarize-dvfilter | grep -n 3 <VMName>
		-> Get the vNIC name of stateful firewall
		-> vsipioctl getrules -f <vNIC Name>
			-> You can decrypt the address set
			-> vsipioctl getaddrset -f <vNIC Name>
				-> This will show the IP and MAC address

Syslog	-> NSX Manager and Edge Node
		-> set logging-server <IP_ADDR> proto <udp/tcp/tls> level <emer/alert/crit/err/warning/notice/info/debug>
	-> ESXi
		-> esxcli system syslog config set --loghost=udp://<IP_ADDR>  --log-level=debug
		-> esxcli system syslog reload
		-> esxcli system syslog config get

Password Management	-> NSX Manager
			-> get auth-policy cli lockout-period/max-auth-failures
			-> set user <username> password-expiration <1-9999>
			-> clear user <username> password-expiration


Edge-bridge Profile	-> HA for Edge Cluster
			-> Premptive	-> Move all the tasks once node is back
			-> Non-premptive -> wait until another node is up and running fine


Edge Cluster	-> Deploy new Edge Node
		-> Go to Edge Cluster -> Go to Action -> Replace Edge Node from to
