NSX 4.0	-> Merged Management & Control Plane
	-> KVM Hosts not supported
	-> vDS can use with NSX if vSphere and vDS version is 7 or newer
	-> Corfu DB is present with synchronous replication in NSX Manager with latency of 10ms

NSX Management Cluster	-> NSX Manager Network (MGMT)
				-> Depends on Appliance Proxy Hub service which is running on NSX Manager
				-> Communicates with Management Plan Agent Deamon which depends upon NSX-Proxy service
				-> Depend upon Port 1234
				-> Used for Stats and Health Status
				-> Communicates over Managemnet network

			-> Central Control Plane (CCP)
				-> Depends on Appliance Proxy Hub service which is running on NSX Manager
				-> Communicates with Local Control Plane Deamon which depends upon NSX-Proxy service
				-> Depend upon Port 1235
				-> Used for Rules, Group, Policies
				-> Communicates over Managemnet network

#============================================================================================================================================================================#

Virtual IP	-> In-Built 	-> You can set the virtual IP Address using NSX Manaager GUI
				-> All Manager should be part of same subnet
				-> It does not work as Load Balancer but the VIP will be mapped to one of the NSX Manaager
		-> External	-> You need external Load balancer (Ex. NSX ALB)
				-> All NSX Managers can be part of different subnets
				-> Balances the load as per the hashing algorithm set in LB

#============================================================================================================================================================================#

NSX Manager Size	-> Small [4vCPU, 16GB RAM, 300GB Storage]	-> POC
			-> Medium [6vCPU, 24GB RAM, 300GB Storage]	-> Less than 128 Transport Nodes
			-> Large [12vCPU, 48GB RAM, 300GB Storage]	-> Equal to or more than 128 Transport Nodes

#============================================================================================================================================================================#

NSX Manager Cluster Deployment methods
	With Compute Manager	-> Deploy first NSX Manager
				-> Add your vCenter environment as compute manager
				-> Deploy additional Appliances on vCenter Environment

	Without Compute Manager	-> Deploy all the 3 NSX Managers as standalone single node NSX Manager Cluster
				-> You will not be able to perform it via GUI as it requires Compute Manager
				-> Hence we will follow CLI Method
				-> Take Putty session of the 1st NSX Manager
					-> Run command "get cluster status" to get the Cluster-ID
					-> Run command "get certificate api thumbprint" to get Thumbprint
				-> Take Putty session of the 2nd NSX Manager
					-> Run below command
					-> Join <1st_NSX_Manager_IP> cluster-id <Cluster-ID> thumbprint <Thumbprint> username <username> password <password>
				-> By doing this you will erase all the data from 2nd NSX Manager and add it in a cluster with 1st NSX Manager
				-> Wait until all the respective services are up and cluster status is stable
				-> You can check the cluster status using "get cluster status"
				-> Once cluster is stable, you can perform the same steps for 3rd NSX Manager

#============================================================================================================================================================================#

NSX Manager Services which are affects the stability of the NSX Manager Cluster
	-> Datastore
	-> Cluster_Boot_Manager
	-> Controller
	-> Manager
	-> HTTPs
	-> Messaging Manager
	-> Site Manager
	-> Monitoring
	-> IDPS Reporting
	-> CM Inventory
	-> Corfu Nonconfig

#============================================================================================================================================================================#

For Overlay traffic, MTU on vDS need to be at least 1600, 1700 (Recommended) or 9000 (Jumbo Frame)
Uplink Profiles	-> No. of Uplinks and Teaming policy defines the number of TEPs created for Transport Node
			-> Failover	-> Only one TEP
			-> Load Balance Source [ESXi Transport Nodes Only]
				-> Balance as per vDS Port ID
					-> Active - Active
					-> Multiple TEPs
				-> Balance as per MAC
					-> Balance as per MAC Hash
					-> High Compute
After the ESXi Node is convereted to Transport Node the respective VMKernal Adapter will get created as per the active Uplinks (i.e. 1 Active Uplink = 1 VMKernel Adapter) for Overlay Traffic.
One more additional VMKernel Adapter will be created for NSX Hyperbus which is related to Containers networking.

#============================================================================================================================================================================#

Segments	-> Single Broadcast domain (Similar to vLAN)
		-> VM's are connected to Segment ports
		-> Two type's of Segments
			-> vLAN Backed segments
				-> for vLAN traffic
				-> connectivity for Physical Network using Edge and T0 Gateways
			-> Overlay Segments
				-> Each Overlay segments has 24 bit long unique VNI (Virtual Network Identifier) ID
				-> for GENEVE (Generic Network Virtualization Encapsulation) traffic
					-> Provide L2 over L3 Encapsulation (Able to Virtualize L2 over L3)
					-> GENEVE uses vTEPs or TEPs (Tunnel End Points)
					-> Encapsulation and Decapsulation happens at TEP's
					-> GENEVE uses 6081 UDP Port
					-> GENEVE has extra variable length options unlike VXLAN
		-> VM's connected to same segment can communicate with each other
		-> VM's on different segments must use Gateway for communication

#============================================================================================================================================================================#

Traffic Flow with Only Segment in Place over the Overlay Network

GENEVE Header	-> Same Physical Layer 2 (Switch) / Same RACK
			-> Segment VNI
			-> Source TEP IP/MAC
			-> Destination TEP IP/MAC
		-> Different Physical Layer 2 (Switch) / Different RACK
			-> Before reaching to Gateway Router
				-> Segment VNI
				-> Source TEP IP/MAC
				-> Destination TEP IP
				-> Gateway Router Interface MAC Address as Destination MAC
			-> After reaching to Gateway Router
				-> Segment VNI
				-> Source TEP IP
				-> Destination TEP IP/MAC
				-> Gateway Router Interface MAC Address as Source MAC

#============================================================================================================================================================================#

BUM (Boradcast, Unicase, Multicast)
ARP Traffic	-> First Source ESX will check the different available table data
		-> Then try to communicate with NSX Manager (Central Control Plane) to get the latest table data over Management Network
		-> If both the time Table does not contain the required information about the destination VM then it will use two different ARP Methods
			-> Head Replication (Lot of ARP Traffic in Physical Network)
				-> Source ESX will communicate with all the ESX Nodes mapped with the respective Segment and has Powered On/Running VM on them via TEP's Encapsulation/Decapsulation
			-> 2 Tier Replication (Comparatively less ARP Traffic in Physical Network)
				-> Source ESX will communicate with same Physical Layer 2 (Switch) segments / Same RACK ESX nodes mapped with the respective Segment and has Powered On/Running VM on them via TEP's Encapsulation/Decapsulation
				-> For different Physical Layer 2 (Switch) segments / Different RACK ESX nodes
					-> It will select one Proxy (mTEP) and ask it to replicate the ARP packet in their respective Physical Layer 2 (Switch) segment/RACK ESX Nodes

#============================================================================================================================================================================#

NSX Forwarding Tables
Tables will not get populated if VMs are not in running state on respective Segments
	-> If Central Control Plane is down
		-> The Communication will work flawlessly as LCP has copy of these tables
		-> Newly powered on VM will not be able to communicate unless the TEP IP entry is populated for that particular ESX nodes vTEP IP in TEP Table
		-> We can not perform the vMotion as it should be mapped with new ESX (Transport) Node

Different Types of Tables
TEP Table	-> ESX (Transport_ Nodes knows about how to communicate with each other due to data available in this
		-> Segment VNI
		-> vTEP MAC
		-> vTEP IPs
		-> Segment/Network IP Address (for e.g., 10.10.10.0)
MAC Table	-> Segment VNI
		-> VM MAC
		-> vTEP IP
ARP Table	-> ARP Table will get flushed if VM's are not going through any network traffic
		-> Segment VNI
		-> VM IP
		-> VM MAC


You can check the Table data using the CLI Method by taking the Putty session of NSX Manager
	-> Run the command "get logical-switch" to get the VNI's for all the available segments
	-> Run the command "get logical-switch <Segment_VNI> vtep" for vTEP Table
	-> Run the command "get logical-switch <Segment_VNI> mac" for MAC Table
	-> Run the command "get logical-switch <Segment_VNI> arp" for ARP Table

You can also download the vTEP and MAC Table using NSX MAnager GUI Network Segment module
#============================================================================================================================================================================#

Gateway Routing	-> Service Router (SR)
			-> Host Centralized services i.e. NAT, Load Balancing, VPN, etc.
			-> Located on Edge Transport Node
			-> Used to provide routing to Physical World

		-> Distributed Router (DR)
			-> DR Component only used at source Transport Node
			-> Deployed and located on every Transport Node
			-> Provides East-west Routing
			-> Runs as a kernel module
			-> No hairpinning while performing traffic route between two segments on same host

For Distributed Router (DR)
	-> Segment connects on LIF (Logical Interface) on Gateways having vMAC and vIP which is same for every LIF
		->vMAC = 02:50:56:56:44:52 (56:44:52 convert from Hex to ASCII = vDR)
	-> The LIF's will have same vIP and vMAC to avoid issue after vMotion of the VM on another host


Automatic Link connections
	-> Tier 0 to Tier 1 Gateway [Linked]
		-> Router Link (100.64.0.0/10)
	-> Service Router (SR) to Distributed Router (DR) [Backplane]
		-> Intra-Tier Transit Link (169.254.0.0/28)
		-> It has auto generated VNIs (65,536 - 75,000)

#============================================================================================================================================================================#

Edge Node	-> Intel, AMD EPYC, anything else fail (for e.g., Ryzen)
		-> Physical (Bare Metal)
			-> Speed depends upon the Physical NIC
			-> Fast failure detection
				-> BFD (Bidirectional Forward Detection) [Hello Packet]
					-> 50ms * 3 = 150ms to detect
			-> Upto 16 Physical NICs

		-> VM
			-> vmxnet 3 driver approx. 10 GBps
			-> Slow failure detection
				-> BFD (Bidirectional Forward Detection) [Hello Packet]
					-> 500ms * 3 = 1500ms to detect
			-> Contains 5 Virtual NICs (4 prior to 3.2.1)
				-> 1st NIC
					-> eth0 (Linux IP Stack)
				-> remaining
					-> fp-eth0 (fast path ethernet)0,1,2,3
						-> 2 for Uplink Traffic
						-> 2 for overlay Traffic

If T0 HA Mode is active active, then we can not run centralized services (NAT, VPN, Firewall)
Now after NSX 4.0.11 T1 supports the stateful services

Edge-bridge Profile	-> HA for Edge Cluster
			-> Premptive
				-> Move all the tasks once node is back
			-> Non-premptive
				-> Manual intervention is required to make standby node as active

Edge Cluster	-> Deploy new Edge Node
		-> Go to Edge Cluster -> Go to Action -> Replace Edge Node from to
#============================================================================================================================================================================#

Troubleshooting NSX Manager Connectivity
		-> Start with Monitoring Dashboards available in NSX Manager GUI
		-> Check whether 1234 / 1235 is listening on ESX (Transport) Node
			-> Take SSH of the ESX Node
			-> Run command "esxcli network ip connection list | grep 1234", this will show all the available NSX Managers
			-> Run command "esxcli network ip connection list | grep 1235", this will show only one NSX Manager acting as Central Control Plane due to sharding meaning each NSX Manager will manage set of ESX (Transport) Node

		-> If ESX (Transport) Node is not listening on port 1234 and 1235 then check the respective services especially "NSX-Proxy" service
		-> Run command "/etc/init.d/nsx-proxy status" to get the status of the service


Troubleshooting Tunnels	-> Tunnels will not get established if
				-> ESX (Transport) Node is in maintenance mode
				-> VMs are not in powered on state on that respective ESX (Transport) Node
				-> VMs running on the ESX (Transport) Node is not using NSX Segment
				-> VMs are not present on the respective ESX (Transport) Node
			-> 1st by looking at the Hosts section of GUI try to identify the faulty ESX (Transport) Node
			-> Now, take the SSH of the faulty ESX (Transport) Node
			-> Run command "esxcfg-vmknic --list" to list down VMKernel ports
			-> Run command "vmkping ++netstack=vxlan <TEP_IP_OF_OTHER_NODE>", this will allow you to identify whether the VMKernel port is up or not
			-> Run command "vmkping ++netstack=vxlan <TEP_IP_OF_OTHER_NODE> -s <Size_of_Packet> -d"
				-> This will allow you to specify the Packet Size to check whether Jumbo Packets are allowed or not without defragmenting it
				-> Note TCP and ICMP header is of 28 bytes so subtract that and then only put the Size (for e.g., if you want to set the size 1600 then mention 1572)


Troubleshooting Routing	-> 1st use Traceflow under "Plan & Troubleshoot" module of NSX Manager GUI from both Policy and Manager interface
				-> If Policy and Manager Interface is not visible
					-> then go to "System" -> "General Settings"
					-> Set "Toggle Visibility" to "Visible to All Users" 
			-> If you try to connect to Physical world then this method will only check till Tier 0 i.e. Edge Node on which Tier0 is located
			-> If everything is green till NSX Edge part then check the Routing tables
				-> Take the SSH of the Edge Node on which Service Router is hosted
				-> Run command "get-logicalrouter", this will show all the logical routers and their VRFs
				-> Run command "vrf <VRF_of_SR>", this will take you inside the respective VRF
				-> Run command "get route", this will show you all the routes and thier via interface/gateway
				-> Verify whether the respective segment with which VM is mapped is visible under route section or not
			-> If the segment is not visible under "get route" command output
				-> then the problem is route distribution
					-> Tier 1
						-> Route publishing
					-> Tier 0
						-> Route advertisement
			-> Also if everything seems okay, then verify whether the Edge Node is mapped with both Overlay and vLAN Transport Zone
			-> Edge and Transport Node should use same Overlay Transport Zone, then only the communication will work between them for North South Traffic forwarding


Troubleshoot Dropped Packets	-> Take SSH of NSX Manager
				-> Run command "get interface"
				-> Run command "get interface"

#============================================================================================================================================================================#

Packet Capture at the NSX Edge Node
	-> Take SSH of the NSX Edge Node using admin user
	-> Run command "get interfaces"
		-> This will show all the available interfaces
		-> In case the Edge node is VM then you will find eth0, fp-eth0, fp-eth1, fp-eth2, fp-eth3 interfaces

	-> Run command "start capture interface <fp-eth0> count 20 file <mycapture>"
		-> This will save the 20 lines of traffic passing via fp-eth0 interface in mycapture file located under NSX Managers "/image/vmware/nsx/file-store" path
		-> You can open this saved file in wireshark for further analysis

	-> Run command "start capture interface <fp-eth0> count 5 expression ip <IP_Address>"
		-> This will show the first 5 lines of traffic passing via fp-eth0 interface containing the respective IP Address

	-> The output of above commands will vary once you change the interface from vLAN to Overlay
	-> Overlay traffic packet contains
		-> Timestamp
		-> GENEVE Header
			-> Source and Destination TEP MAC
			-> Source and Destination TEP IP
			-> VNI (Hexa value)
				-> You can convert it to Decimal and match it with the output of 
					-> "get logical-routers"
						-> "vrf <VRF_of_SR>"
						-> "get interface"
						-> The backplane (DR to SR connection) VNI
					-> "get logical-switch"
				-> BFD packets has VNI of 0x0
		-> Actual Packet
		-> Payload data
		 
#============================================================================================================================================================================#


Reverse Proxy Service -> NSX Manager service for registering Transport Node

esxcli software vib list | grep -E 'nsx|vsip'
vmkping -I (Interface) -s (Size of Package) -d (not defragment) -S vxlan (type of packate) (destination)

Intratier Transit Link -> T0 to T1

The Data Plane Development Kit (DPDK) is a set of data plane libraries and network interface controller drivers for fast packet processing
DFW 	-> Ethernet -> L2 Rule
	-> Emergency -> Quaratine Purpose
	-> Infrastructure -> DNS, DHCP, AD
	-> Environment -> based on type (Prod, Non-Prod)
	-> Application -> Layer 7
	-> on ESXi Host
		-> summarize-dvfilter | grep -n 3 <VMName>
		-> Get the vNIC name of stateful firewall
		-> vsipioctl getrules -f <vNIC Name>
			-> You can decrypt the address set
			-> vsipioctl getaddrset -f <vNIC Name>
				-> This will show the IP and MAC address

Syslog	-> NSX Manager and Edge Node
		-> set logging-server <IP_ADDR> proto <udp/tcp/tls> level <emer/alert/crit/err/warning/notice/info/debug>
	-> ESXi
		-> esxcli system syslog config set --loghost=udp://<IP_ADDR>  --log-level=debug
		-> esxcli system syslog reload
		-> esxcli system syslog config get

Password Management	-> NSX Manager
			-> get auth-policy cli lockout-period/max-auth-failures
			-> set user <username> password-expiration <1-9999>
			-> clear user <username> password-expiration






